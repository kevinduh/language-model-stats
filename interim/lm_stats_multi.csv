Languages,Organisation,Model,Citation,URL,Architecture,Data,Tokens (B),Size (B),Compute,Training Strategy
ml,Google,Mbert,[Devlin 2019](https://arxiv.org/pdf/1810.04805.pdf),https://arxiv.org/pdf/1810.04805.pdf,EncDec,Wikipedia,?,0.1,?,"MLM, Next Sentence Prediction"
ml,Meta,MBART-large,[Liu 2020](https://arxiv.org/pdf/2001.08210.pdf),https://arxiv.org/pdf/2001.08210.pdf,Enc-Dec,"CC25, backtranslate",?,0.6,256 Nvidia V100 GPUs for 18 days,"MLM; (Bart training), sentence shuffling"
ml,Meta,XGLM,[Lin 2021](https://arxiv.org/pdf/2112.10668.pdf),https://arxiv.org/pdf/2112.10668.pdf,EncDec,CC100,500,7.5,v100s for ?,AR; Trained on mixture of monoligual texts
ml,Meta,XLMR,[Goyal 2021](https://arxiv.org/pdf/2105.00572.pdf),https://arxiv.org/pdf/2105.00572.pdf,Enc-Dec,CC100,167,10.7,,MLM
ml,Google,Byt5,[Xue 2022](https://arxiv.org/pdf/2105.13626.pdf),https://arxiv.org/pdf/2105.13626.pdf,Enc-Dec,MC4,6.4T,12.9,?,Token free (byte level MT5)
ml,Google,MT5,[Xue 2021](https://arxiv.org/pdf/2010.11934.pdf),https://arxiv.org/pdf/2010.11934.pdf,Enc-Dec,MC4,6.4T,13,?,MLM
ml,Meta,M2M100 ,[Fan 2020](https://arxiv.org/pdf/2010.11125.pdf),https://arxiv.org/pdf/2010.11125.pdf,Enc-Dec,"CCMatrix, backtranslate",7.5 parallel sentences; ,15.4,"""hundreds of GPUs"" ",AR
