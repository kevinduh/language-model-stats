Languages,Organisation,Model,Citation,URL,Architecture,Data,Tokens (B),Size (B),Compute,Training Strategy
en,Google,BERT Large,Devlin 2019,https://aclanthology.org/N19-1423.pdf,EncDec,"Books, Wikipedia",3.3,0.34,64TPUs for 4 days,"MLM, Next sentence prediction"
en,Meta,BART,Lewis 2019,https://arxiv.org/pdf/1910.13461.pdf,EncDec,"Books, Wikipedia (160Gb)",,0.4,64 TPU chips for 4 days,MLM scheme where multiple tokens are masked with a single [MASK] token
en,Google,XLNet,Yang 2019,https://arxiv.org/pdf/1906.08237.pdf,Dec,"Books, Wikipedia, ClueWeb, CommonCrawl",33,0.4, 512 TPU v3 chips for 5 days,Permutation 
en,Meta,Blenderbot3B,Roller 2020,https://arxiv.org/pdf/2004.13637.pdf,EncDec,"Reddit, ConvAI, Empathetic Dialogues, WoW, Blended Skill Talk",88.8,2.7B,?,"MLM, 2 step; first retrieve then concatenate to input to generate a refined response"
en,EleutherAI,GPTNeo,Black 2021,https://github.com/EleutherAI/gpt-neo,Dec,The Pile,420,2.7,?,AR
en,Google,T5,Raffel 2020 ,https://arxiv.org/pdf/1910.10683.pdf,EncDec,C4 (cleaned common crawl),34,11,"1,024 TPU v3 chips for ? ",Combines pre-training followed by fine-tuning on multiple tasks using the same architecture
en,OpenAI,GPT-3,Brown 2020,https://arxiv.org/pdf/2005.14165.pdf,Dec,"Books, Wikipedia, Webcrawl",300 (400?),175,?,AR
en,Meta,OPT,Zhang 2022,https://arxiv.org/pdf/2205.01068.pdf,Dec,"Books, CCNews, The Pile, Reddit",300,175B,992 80GB A100 GPUs for ? ,AR
ml,Google,Mbert,Devlin 2019,https://github.com/google-research/bert/blob/master/multilingual.md,EncDec,Wikipedia,?,0.1,?,"MLM, Next Sentence Prediction"
ml,Meta,MBART-large,Liu 2020,https://arxiv.org/pdf/2001.08210.pdf,Enc-Dec,"CC25, backtranslate",?,0.6,256 Nvidia V100 GPUs for 18 days,"MLM; (Bart training), sentence shuffling"
ml,Meta,XGLM,Lin 2021,https://arxiv.org/pdf/2112.10668.pdf,EncDec,CC100,500,7.5,v100s for ?,AR; Trained on mixture of monoligual texts
ml,Meta,XLMR,Goyal 2020,https://arxiv.org/pdf/1911.02116.pdf,Enc-Dec,CC100,167,10.7,N/A,MLM
ml,Google,Byt5,Xue 2022,https://arxiv.org/pdf/2105.13626.pdf,Enc-Dec,MC4,6.4T,12.9,?,Token free (byte level MT5)
ml,Google,MT5,Xue 2021,https://arxiv.org/pdf/2010.11934.pdf,Enc-Dec,MC4,6.4T,13,?,MLM
ml,Meta,M2M100 ,Fan 2020,https://arxiv.org/pdf/2010.11125.pdf,Enc-Dec,"CCMatrix, backtranslate",7.5 parallel sentences; ,15.4,"""hundreds of GPUs"" ",AR
